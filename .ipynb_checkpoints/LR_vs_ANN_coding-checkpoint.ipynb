{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.893365562058 0.869586257938\n",
      "1000 0.0926885822914 0.071289687979\n",
      "2000 0.0886053816977 0.0691349995181\n",
      "3000 0.0869820889476 0.0689398479222\n",
      "4000 0.086126624244 0.0691583350821\n",
      "5000 0.0856073470752 0.0694813121411\n",
      "6000 0.0852640004157 0.0698203370935\n",
      "7000 0.08502359292 0.0701463002242\n",
      "8000 0.0848482055078 0.070449678149\n",
      "9000 0.0847162372908 0.0707281982655\n",
      "Final train classification_rate: 0.925\n",
      "Final test classification_rate: 0.84\n"
     ]
    }
   ],
   "source": [
    "# Some ecommerce data Logistic regression\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from process import get_data\n",
    "\n",
    "def y2indicator(y, K):\n",
    "    N = len(y)\n",
    "    ind = np.zeros((N, K))\n",
    "    # basically matrix with only label corresponding to n in Y is 1\n",
    "    for i in xrange(N):\n",
    "        ind[i, y[i]] = 1\n",
    "    return ind\n",
    "\n",
    "X, Y = get_data()\n",
    "X, Y = shuffle(X, Y)\n",
    "Y = Y.astype(np.int32)\n",
    "D = X.shape[1]\n",
    "K = len(set(Y))\n",
    "\n",
    "# create train and test sets\n",
    "Xtrain = X[:-100]\n",
    "Ytrain = Y[:-100]\n",
    "Ytrain_ind = y2indicator(Ytrain, K)\n",
    "Xtest = X[-100:]\n",
    "Ytest = Y[-100:]\n",
    "Ytest_ind = y2indicator(Ytest, K)\n",
    "\n",
    "# randomly initialize weights\n",
    "W = np.random.randn(D, K)\n",
    "b = np.zeros(K)\n",
    "\n",
    "# make predictions\n",
    "def softmax(a):\n",
    "    expA = np.exp(a)\n",
    "    return expA / expA.sum(axis=1, keepdims=True)\n",
    "\n",
    "def forward(X, W, b):\n",
    "    return softmax(X.dot(W) + b)\n",
    "\n",
    "def predict(P_Y_given_X):\n",
    "    return np.argmax(P_Y_given_X, axis=1)\n",
    "\n",
    "# calculate the accuracy\n",
    "def classification_rate(Y, P):\n",
    "    return np.mean(Y == P)\n",
    "\n",
    "def cross_entropy(T, pY):\n",
    "    return -np.mean(T*np.log(pY))\n",
    "\n",
    "\n",
    "# train loop\n",
    "train_costs = []\n",
    "test_costs = []\n",
    "learning_rate = 0.001\n",
    "for i in xrange(10000):\n",
    "    pYtrain = forward(Xtrain, W, b)\n",
    "    pYtest = forward(Xtest, W, b)\n",
    "\n",
    "    ctrain = cross_entropy(Ytrain_ind, pYtrain)\n",
    "    ctest = cross_entropy(Ytest_ind, pYtest)\n",
    "    train_costs.append(ctrain)\n",
    "    test_costs.append(ctest)\n",
    "\n",
    "    # gradient descent\n",
    "    W -= learning_rate*Xtrain.T.dot(pYtrain - Ytrain_ind)\n",
    "    b -= learning_rate*(pYtrain - Ytrain_ind).sum(axis=0)\n",
    "    if i % 1000 == 0:\n",
    "        print i, ctrain, ctest\n",
    "\n",
    "print \"Final train classification_rate:\", classification_rate(Ytrain, predict(pYtrain))\n",
    "print \"Final test classification_rate:\", classification_rate(Ytest, predict(pYtest))\n",
    "\n",
    "legend1, = plt.plot(train_costs, label='train cost')\n",
    "legend2, = plt.plot(test_costs, label='test cost')\n",
    "plt.legend([legend1, legend2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.81419795438 0.832547563743\n",
      "1000 0.0250557802829 0.0396334416363\n",
      "2000 0.0188436320604 0.0388774925957\n",
      "3000 0.0168056946495 0.0409206274332\n",
      "4000 0.0157159743825 0.0430514291297\n",
      "5000 0.0149977529681 0.045109819454\n",
      "6000 0.0144696341421 0.047061998296\n",
      "7000 0.0140545006817 0.0488813230914\n",
      "8000 0.0137121044082 0.0505428232917\n",
      "9000 0.013413150203 0.0520135614766\n",
      "Final train classification_rate: 0.9825\n",
      "Final test classification_rate: 0.96\n"
     ]
    }
   ],
   "source": [
    "## Same data ANN\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from process import get_data\n",
    "\n",
    "def y2indicator(y, K):\n",
    "    N = len(y)\n",
    "    ind = np.zeros((N, K))\n",
    "    for i in xrange(N):\n",
    "        ind[i, y[i]] = 1\n",
    "    return ind\n",
    "\n",
    "X, Y = get_data()\n",
    "X, Y = shuffle(X, Y)\n",
    "Y = Y.astype(np.int32)\n",
    "M = 5\n",
    "D = X.shape[1]\n",
    "K = len(set(Y))\n",
    "\n",
    "# create train and test sets\n",
    "Xtrain = X[:-100]\n",
    "Ytrain = Y[:-100]\n",
    "Ytrain_ind = y2indicator(Ytrain, K)\n",
    "Xtest = X[-100:]\n",
    "Ytest = Y[-100:]\n",
    "Ytest_ind = y2indicator(Ytest, K)\n",
    "\n",
    "# randomly initialize weights\n",
    "W1 = np.random.randn(D, M)\n",
    "b1 = np.zeros(M)\n",
    "W2 = np.random.randn(M, K)\n",
    "b2 = np.zeros(K)\n",
    "\n",
    "# make predictions\n",
    "def softmax(a):\n",
    "    expA = np.exp(a)\n",
    "    return expA / expA.sum(axis=1, keepdims=True)\n",
    "\n",
    "def forward(X, W1, b1, W2, b2):\n",
    "    Z = np.tanh(X.dot(W1) + b1)\n",
    "    return softmax(Z.dot(W2) + b2), Z\n",
    "\n",
    "def predict(P_Y_given_X):\n",
    "    return np.argmax(P_Y_given_X, axis=1)\n",
    "\n",
    "# calculate the accuracy\n",
    "def classification_rate(Y, P):\n",
    "    return np.mean(Y == P)\n",
    "\n",
    "def cross_entropy(T, pY):\n",
    "    return -np.mean(T*np.log(pY))\n",
    "\n",
    "\n",
    "# train loop\n",
    "train_costs = []\n",
    "test_costs = []\n",
    "learning_rate = 0.001\n",
    "for i in xrange(10000):\n",
    "    pYtrain, Ztrain = forward(Xtrain, W1, b1, W2, b2)\n",
    "    pYtest, Ztest = forward(Xtest, W1, b1, W2, b2)\n",
    "\n",
    "    ctrain = cross_entropy(Ytrain_ind, pYtrain)\n",
    "    ctest = cross_entropy(Ytest_ind, pYtest)\n",
    "    train_costs.append(ctrain)\n",
    "    test_costs.append(ctest)\n",
    "\n",
    "    # gradient descent\n",
    "    W2 -= learning_rate*Ztrain.T.dot(pYtrain - Ytrain_ind)\n",
    "    b2 -= learning_rate*(pYtrain - Ytrain_ind).sum()\n",
    "    dZ = (pYtrain - Ytrain_ind).dot(W2.T) * (1 - Ztrain*Ztrain)\n",
    "    W1 -= learning_rate*Xtrain.T.dot(dZ)\n",
    "    b1 -= learning_rate*dZ.sum(axis=0)\n",
    "    if i % 1000 == 0:\n",
    "        print i, ctrain, ctest\n",
    "\n",
    "print \"Final train classification_rate:\", classification_rate(Ytrain, predict(pYtrain))\n",
    "print \"Final test classification_rate:\", classification_rate(Ytest, predict(pYtest))\n",
    "\n",
    "legend1, = plt.plot(train_costs, label='train cost')\n",
    "legend2, = plt.plot(test_costs, label='test cost')\n",
    "plt.legend([legend1, legend2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
