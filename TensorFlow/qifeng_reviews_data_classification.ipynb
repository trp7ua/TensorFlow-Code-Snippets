{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 946470.458333333\n",
      "Epoch: 0002 cost= 912871.520833333\n",
      "Epoch: 0003 cost= 879281.468750000\n",
      "Epoch: 0004 cost= 845701.375000000\n",
      "Epoch: 0005 cost= 812130.385416667\n",
      "Epoch: 0006 cost= 778568.843750000\n",
      "Epoch: 0007 cost= 745015.062500000\n",
      "Epoch: 0008 cost= 711472.177083333\n",
      "Epoch: 0009 cost= 677942.510416667\n",
      "Epoch: 0010 cost= 644427.864583333\n",
      "Epoch: 0011 cost= 610930.052083333\n",
      "Epoch: 0012 cost= 577448.906250000\n",
      "Epoch: 0013 cost= 543990.989583333\n",
      "Epoch: 0014 cost= 510561.536458333\n",
      "Epoch: 0015 cost= 477163.510416667\n",
      "Epoch: 0016 cost= 443800.765625000\n",
      "Epoch: 0017 cost= 410482.437500000\n",
      "Epoch: 0018 cost= 377215.895833333\n",
      "Epoch: 0019 cost= 344000.130208333\n",
      "Epoch: 0020 cost= 310833.619791667\n",
      "Epoch: 0021 cost= 277705.317708333\n",
      "Epoch: 0022 cost= 244612.570312500\n",
      "Epoch: 0023 cost= 211549.786458333\n",
      "Epoch: 0024 cost= 178512.635416667\n",
      "Epoch: 0025 cost= 145714.002604167\n",
      "Epoch: 0026 cost= 113574.477864583\n",
      "Epoch: 0027 cost= 83285.988281250\n",
      "Epoch: 0028 cost= 61839.552734375\n",
      "Epoch: 0029 cost= 59493.641276042\n",
      "Epoch: 0030 cost= 59631.230468750\n",
      "Epoch: 0031 cost= 59553.777994792\n",
      "Epoch: 0032 cost= 59362.510416667\n",
      "Epoch: 0033 cost= 59111.416015625\n",
      "Epoch: 0034 cost= 58829.453125000\n",
      "Epoch: 0035 cost= 58532.025390625\n",
      "Epoch: 0036 cost= 58227.066406250\n",
      "Epoch: 0037 cost= 57918.695312500\n",
      "Epoch: 0038 cost= 57609.036458333\n",
      "Epoch: 0039 cost= 57299.132161458\n",
      "Epoch: 0040 cost= 56989.453125000\n",
      "Epoch: 0041 cost= 56680.295572917\n",
      "Epoch: 0042 cost= 56371.656901042\n",
      "Epoch: 0043 cost= 56063.512369792\n",
      "Epoch: 0044 cost= 55755.669270833\n",
      "Epoch: 0045 cost= 55447.964843750\n",
      "Epoch: 0046 cost= 55140.285807292\n",
      "Epoch: 0047 cost= 54832.387369792\n",
      "Epoch: 0048 cost= 54523.302734375\n",
      "Epoch: 0049 cost= 54211.248697917\n",
      "Epoch: 0050 cost= 53928.262369792\n",
      "Epoch: 0051 cost= 53658.600260417\n",
      "Epoch: 0052 cost= 53386.483723958\n",
      "Epoch: 0053 cost= 53115.736979167\n",
      "Epoch: 0054 cost= 52841.974609375\n",
      "Epoch: 0055 cost= 52568.608723958\n",
      "Epoch: 0056 cost= 52295.305338542\n",
      "Epoch: 0057 cost= 52032.419921875\n",
      "Epoch: 0058 cost= 51805.602864583\n",
      "Epoch: 0059 cost= 51578.984375000\n",
      "Epoch: 0060 cost= 51348.333333333\n",
      "Epoch: 0061 cost= 51121.003906250\n",
      "Epoch: 0062 cost= 50893.290364583\n",
      "Epoch: 0063 cost= 50665.429036458\n",
      "Epoch: 0064 cost= 50438.080078125\n",
      "Epoch: 0065 cost= 50210.764973958\n",
      "Epoch: 0066 cost= 49983.321614583\n",
      "Epoch: 0067 cost= 49755.828776042\n",
      "Epoch: 0068 cost= 49528.229166667\n",
      "Epoch: 0069 cost= 49300.423828125\n",
      "Epoch: 0070 cost= 49072.369791667\n",
      "Epoch: 0071 cost= 48844.089843750\n",
      "Epoch: 0072 cost= 48615.626302083\n",
      "Epoch: 0073 cost= 48386.955729167\n",
      "Epoch: 0074 cost= 48158.046223958\n",
      "Epoch: 0075 cost= 47929.064453125\n",
      "Epoch: 0076 cost= 47699.712239583\n",
      "Epoch: 0077 cost= 47469.911458333\n",
      "Epoch: 0078 cost= 47239.652994792\n",
      "Epoch: 0079 cost= 47008.975260417\n",
      "Epoch: 0080 cost= 46777.855468750\n",
      "Epoch: 0081 cost= 46546.387369792\n",
      "Epoch: 0082 cost= 46314.570312500\n",
      "Epoch: 0083 cost= 46082.391276042\n",
      "Epoch: 0084 cost= 45849.789713542\n",
      "Epoch: 0085 cost= 45616.614583333\n",
      "Epoch: 0086 cost= 45382.928385417\n",
      "Epoch: 0087 cost= 45148.559244792\n",
      "Epoch: 0088 cost= 44913.504557292\n",
      "Epoch: 0089 cost= 44677.835937500\n",
      "Epoch: 0090 cost= 44441.521484375\n",
      "Epoch: 0091 cost= 44204.561197917\n",
      "Epoch: 0092 cost= 43966.912760417\n",
      "Epoch: 0093 cost= 43728.850911458\n",
      "Epoch: 0094 cost= 43490.248697917\n",
      "Epoch: 0095 cost= 43251.188802083\n",
      "Epoch: 0096 cost= 43011.641276042\n",
      "Epoch: 0097 cost= 42771.518880208\n",
      "Epoch: 0098 cost= 42530.889322917\n",
      "Epoch: 0099 cost= 42289.699869792\n",
      "Epoch: 0100 cost= 42047.951822917\n",
      "Epoch: 0101 cost= 41805.718750000\n",
      "Epoch: 0102 cost= 41568.557942708\n",
      "Epoch: 0103 cost= 41339.542968750\n",
      "Epoch: 0104 cost= 41108.767578125\n",
      "Epoch: 0105 cost= 40877.377604167\n",
      "Epoch: 0106 cost= 40644.935546875\n",
      "Epoch: 0107 cost= 40411.395833333\n",
      "Epoch: 0108 cost= 40176.904947917\n",
      "Epoch: 0109 cost= 39941.453776042\n",
      "Epoch: 0110 cost= 39704.998046875\n",
      "Epoch: 0111 cost= 39467.198567708\n",
      "Epoch: 0112 cost= 39228.343750000\n",
      "Epoch: 0113 cost= 38988.136718750\n",
      "Epoch: 0114 cost= 38746.942057292\n",
      "Epoch: 0115 cost= 38504.757812500\n",
      "Epoch: 0116 cost= 38261.093750000\n",
      "Epoch: 0117 cost= 38015.805338542\n",
      "Epoch: 0118 cost= 37768.924479167\n",
      "Epoch: 0119 cost= 37520.658203125\n",
      "Epoch: 0120 cost= 37270.895182292\n",
      "Epoch: 0121 cost= 37019.558593750\n",
      "Epoch: 0122 cost= 36766.925130208\n",
      "Epoch: 0123 cost= 36513.064453125\n",
      "Epoch: 0124 cost= 36257.612630208\n",
      "Epoch: 0125 cost= 36000.410156250\n",
      "Epoch: 0126 cost= 35741.113932292\n",
      "Epoch: 0127 cost= 35479.504557292\n",
      "Epoch: 0128 cost= 35215.535156250\n",
      "Epoch: 0129 cost= 34949.041666667\n",
      "Epoch: 0130 cost= 34680.196289062\n",
      "Epoch: 0131 cost= 34409.363281250\n",
      "Epoch: 0132 cost= 34136.516276042\n",
      "Epoch: 0133 cost= 33861.665364583\n",
      "Epoch: 0134 cost= 33584.097656250\n",
      "Epoch: 0135 cost= 33303.920898438\n",
      "Epoch: 0136 cost= 33020.993489583\n",
      "Epoch: 0137 cost= 32736.000651042\n",
      "Epoch: 0138 cost= 32448.165690104\n",
      "Epoch: 0139 cost= 32158.454427083\n",
      "Epoch: 0140 cost= 31866.380859375\n",
      "Epoch: 0141 cost= 31572.401692708\n",
      "Epoch: 0142 cost= 31275.547851562\n",
      "Epoch: 0143 cost= 30976.993815104\n",
      "Epoch: 0144 cost= 30675.454752604\n",
      "Epoch: 0145 cost= 30372.285156250\n",
      "Epoch: 0146 cost= 30066.319335938\n",
      "Epoch: 0147 cost= 29759.371419271\n",
      "Epoch: 0148 cost= 29449.156575521\n",
      "Epoch: 0149 cost= 29136.504882812\n",
      "Epoch: 0150 cost= 28820.993164062\n",
      "Epoch: 0151 cost= 28503.647460937\n",
      "Epoch: 0152 cost= 28183.899088542\n",
      "Epoch: 0153 cost= 27862.140299479\n",
      "Epoch: 0154 cost= 27537.672851562\n",
      "Epoch: 0155 cost= 27210.973958333\n",
      "Epoch: 0156 cost= 26881.715494792\n",
      "Epoch: 0157 cost= 26550.840494792\n",
      "Epoch: 0158 cost= 26217.394205729\n",
      "Epoch: 0159 cost= 25881.868489583\n",
      "Epoch: 0160 cost= 25543.280598958\n",
      "Epoch: 0161 cost= 25202.743164062\n",
      "Epoch: 0162 cost= 24859.455729167\n",
      "Epoch: 0163 cost= 24514.588216146\n",
      "Epoch: 0164 cost= 24166.455403646\n",
      "Epoch: 0165 cost= 23816.521809896\n",
      "Epoch: 0166 cost= 23463.593750000\n",
      "Epoch: 0167 cost= 23108.485677083\n",
      "Epoch: 0168 cost= 22750.246744792\n",
      "Epoch: 0169 cost= 22388.795898438\n",
      "Epoch: 0170 cost= 22023.563476562\n",
      "Epoch: 0171 cost= 21654.614908854\n",
      "Epoch: 0172 cost= 21281.838216146\n",
      "Epoch: 0173 cost= 20905.079752604\n",
      "Epoch: 0174 cost= 20524.511393229\n",
      "Epoch: 0175 cost= 20139.961263021\n",
      "Epoch: 0176 cost= 19751.526367188\n",
      "Epoch: 0177 cost= 19359.072591146\n",
      "Epoch: 0178 cost= 18962.637695312\n",
      "Epoch: 0179 cost= 18562.251302083\n",
      "Epoch: 0180 cost= 18160.316731771\n",
      "Epoch: 0181 cost= 17758.674153646\n",
      "Epoch: 0182 cost= 17358.845703125\n",
      "Epoch: 0183 cost= 16960.753092448\n",
      "Epoch: 0184 cost= 16563.856770833\n",
      "Epoch: 0185 cost= 16170.124511719\n",
      "Epoch: 0186 cost= 15788.977864583\n",
      "Epoch: 0187 cost= 15407.807128906\n",
      "Epoch: 0188 cost= 15027.809244792\n",
      "Epoch: 0189 cost= 14649.813639323\n",
      "Epoch: 0190 cost= 14275.237630208\n",
      "Epoch: 0191 cost= 13903.270996094\n",
      "Epoch: 0192 cost= 13534.896321615\n",
      "Epoch: 0193 cost= 13169.134440104\n",
      "Epoch: 0194 cost= 12805.078776042\n",
      "Epoch: 0195 cost= 12443.432291667\n",
      "Epoch: 0196 cost= 12083.281738281\n",
      "Epoch: 0197 cost= 11722.751302083\n",
      "Epoch: 0198 cost= 11363.804850260\n",
      "Epoch: 0199 cost= 11005.192708333\n",
      "Epoch: 0200 cost= 10645.217122396\n",
      "Epoch: 0201 cost= 10286.603190104\n",
      "Epoch: 0202 cost= 9929.010416667\n",
      "Epoch: 0203 cost= 9573.975748698\n",
      "Epoch: 0204 cost= 9220.521158854\n",
      "Epoch: 0205 cost= 8869.265625000\n",
      "Epoch: 0206 cost= 8519.790852865\n",
      "Epoch: 0207 cost= 8172.219482422\n",
      "Epoch: 0208 cost= 7825.480712891\n",
      "Epoch: 0209 cost= 7476.935546875\n",
      "Epoch: 0210 cost= 7128.137369792\n",
      "Epoch: 0211 cost= 6778.995605469\n",
      "Epoch: 0212 cost= 6432.060465495\n",
      "Epoch: 0213 cost= 6088.914713542\n",
      "Epoch: 0214 cost= 5747.849365234\n",
      "Epoch: 0215 cost= 5410.565673828\n",
      "Epoch: 0216 cost= 5079.103515625\n",
      "Epoch: 0217 cost= 4749.782877604\n",
      "Epoch: 0218 cost= 4424.623453776\n",
      "Epoch: 0219 cost= 4108.423421224\n",
      "Epoch: 0220 cost= 3800.125691732\n",
      "Epoch: 0221 cost= 3501.977945964\n",
      "Epoch: 0222 cost= 3214.753865560\n",
      "Epoch: 0223 cost= 2946.741943359\n",
      "Epoch: 0224 cost= 2698.893147786\n",
      "Epoch: 0225 cost= 2453.069661458\n",
      "Epoch: 0226 cost= 2269.914794922\n",
      "Epoch: 0227 cost= 2108.584574382\n",
      "Epoch: 0228 cost= 1952.200419108\n",
      "Epoch: 0229 cost= 1806.064432780\n",
      "Epoch: 0230 cost= 1666.133239746\n",
      "Epoch: 0231 cost= 1540.462259928\n",
      "Epoch: 0232 cost= 1419.847127279\n",
      "Epoch: 0233 cost= 1306.771545410\n",
      "Epoch: 0234 cost= 1199.533020020\n",
      "Epoch: 0235 cost= 1101.187550863\n",
      "Epoch: 0236 cost= 1009.035542806\n",
      "Epoch: 0237 cost= 924.499165853\n",
      "Epoch: 0238 cost= 845.223927816\n",
      "Epoch: 0239 cost= 772.999267578\n",
      "Epoch: 0240 cost= 705.512044271\n",
      "Epoch: 0241 cost= 642.177846273\n",
      "Epoch: 0242 cost= 584.418029785\n",
      "Epoch: 0243 cost= 531.065058390\n",
      "Epoch: 0244 cost= 482.374954224\n",
      "Epoch: 0245 cost= 438.385645549\n",
      "Epoch: 0246 cost= 398.953862508\n",
      "Epoch: 0247 cost= 362.949289958\n",
      "Epoch: 0248 cost= 330.089080811\n",
      "Epoch: 0249 cost= 300.468200684\n",
      "Epoch: 0250 cost= 273.913088481\n",
      "Epoch: 0251 cost= 250.203956604\n",
      "Epoch: 0252 cost= 229.239448547\n",
      "Epoch: 0253 cost= 210.802096049\n",
      "Epoch: 0254 cost= 195.033233643\n",
      "Epoch: 0255 cost= 181.440877279\n",
      "Epoch: 0256 cost= 169.438344320\n",
      "Epoch: 0257 cost= 158.776433309\n",
      "Epoch: 0258 cost= 149.235664368\n",
      "Epoch: 0259 cost= 140.938760122\n",
      "Epoch: 0260 cost= 133.561290741\n",
      "Epoch: 0261 cost= 126.988726298\n",
      "Epoch: 0262 cost= 121.294425964\n",
      "Epoch: 0263 cost= 116.246387482\n",
      "Epoch: 0264 cost= 111.542479197\n",
      "Epoch: 0265 cost= 107.204302470\n",
      "Epoch: 0266 cost= 103.351571401\n",
      "Epoch: 0267 cost= 99.725798289\n",
      "Epoch: 0268 cost= 96.300278982\n",
      "Epoch: 0269 cost= 93.092880249\n",
      "Epoch: 0270 cost= 90.093658447\n",
      "Epoch: 0271 cost= 87.272089640\n",
      "Epoch: 0272 cost= 84.668134054\n",
      "Epoch: 0273 cost= 82.259977341\n",
      "Epoch: 0274 cost= 80.002539953\n",
      "Epoch: 0275 cost= 77.862778346\n",
      "Epoch: 0276 cost= 75.876223882\n",
      "Epoch: 0277 cost= 74.037068049\n",
      "Epoch: 0278 cost= 72.267488480\n",
      "Epoch: 0279 cost= 70.628292720\n",
      "Epoch: 0280 cost= 69.087067922\n",
      "Epoch: 0281 cost= 67.609188716\n",
      "Epoch: 0282 cost= 66.182783763\n",
      "Epoch: 0283 cost= 64.838816961\n",
      "Epoch: 0284 cost= 63.567624410\n",
      "Epoch: 0285 cost= 62.315358480\n",
      "Epoch: 0286 cost= 61.080352783\n",
      "Epoch: 0287 cost= 59.935757319\n",
      "Epoch: 0288 cost= 58.795408885\n",
      "Epoch: 0289 cost= 57.671586990\n",
      "Epoch: 0290 cost= 56.572141012\n",
      "Epoch: 0291 cost= 55.488888423\n",
      "Epoch: 0292 cost= 54.455032349\n",
      "Epoch: 0293 cost= 53.501198133\n",
      "Epoch: 0294 cost= 52.516279538\n",
      "Epoch: 0295 cost= 51.540350596\n",
      "Epoch: 0296 cost= 50.595019658\n",
      "Epoch: 0297 cost= 49.686008453\n",
      "Epoch: 0298 cost= 48.803493818\n",
      "Epoch: 0299 cost= 47.941551526\n",
      "Epoch: 0300 cost= 47.161345482\n",
      "Epoch: 0301 cost= 46.387165387\n",
      "Epoch: 0302 cost= 45.640970866\n",
      "Epoch: 0303 cost= 44.980110804\n",
      "Epoch: 0304 cost= 44.361629804\n",
      "Epoch: 0305 cost= 43.726581891\n",
      "Epoch: 0306 cost= 43.111237844\n",
      "Epoch: 0307 cost= 42.527602832\n",
      "Epoch: 0308 cost= 41.957431157\n",
      "Epoch: 0309 cost= 41.379645030\n",
      "Epoch: 0310 cost= 40.827184041\n",
      "Epoch: 0311 cost= 40.339876175\n",
      "Epoch: 0312 cost= 39.927090327\n",
      "Epoch: 0313 cost= 39.395203590\n",
      "Epoch: 0314 cost= 38.909408251\n",
      "Epoch: 0315 cost= 38.442490578\n",
      "Epoch: 0316 cost= 38.033970833\n",
      "Epoch: 0317 cost= 37.677334150\n",
      "Epoch: 0318 cost= 37.213618596\n",
      "Epoch: 0319 cost= 36.824431101\n",
      "Epoch: 0320 cost= 36.547439257\n",
      "Epoch: 0321 cost= 36.166385651\n",
      "Epoch: 0322 cost= 35.778642019\n",
      "Epoch: 0323 cost= 35.537329515\n",
      "Epoch: 0324 cost= 35.239826520\n",
      "Epoch: 0325 cost= 34.835684935\n",
      "Epoch: 0326 cost= 34.613722801\n",
      "Epoch: 0327 cost= 34.363265673\n",
      "Epoch: 0328 cost= 33.959917545\n",
      "Epoch: 0329 cost= 33.770677567\n",
      "Epoch: 0330 cost= 33.549257437\n",
      "Epoch: 0331 cost= 33.311861992\n",
      "Epoch: 0332 cost= 33.073901494\n",
      "Epoch: 0333 cost= 32.851012548\n",
      "Epoch: 0334 cost= 32.545634747\n",
      "Epoch: 0335 cost= 32.391524951\n",
      "Epoch: 0336 cost= 32.186237335\n",
      "Epoch: 0337 cost= 31.951480071\n",
      "Epoch: 0338 cost= 31.712990443\n",
      "Epoch: 0339 cost= 31.494606654\n",
      "Epoch: 0340 cost= 31.300029914\n",
      "Epoch: 0341 cost= 31.115851084\n",
      "Epoch: 0342 cost= 30.915170193\n",
      "Epoch: 0343 cost= 30.725261211\n",
      "Epoch: 0344 cost= 30.530461629\n",
      "Epoch: 0345 cost= 30.355595112\n",
      "Epoch: 0346 cost= 30.168561141\n",
      "Epoch: 0347 cost= 29.984510104\n",
      "Epoch: 0348 cost= 29.798058987\n",
      "Epoch: 0349 cost= 29.615192572\n",
      "Epoch: 0350 cost= 29.440906684\n",
      "Epoch: 0351 cost= 29.300635020\n",
      "Epoch: 0352 cost= 29.131250461\n",
      "Epoch: 0353 cost= 28.974902550\n",
      "Epoch: 0354 cost= 28.848035574\n",
      "Epoch: 0355 cost= 28.707647006\n",
      "Epoch: 0356 cost= 28.544424613\n",
      "Epoch: 0357 cost= 28.426406384\n",
      "Epoch: 0358 cost= 28.310836315\n",
      "Epoch: 0359 cost= 28.169093529\n",
      "Epoch: 0360 cost= 28.024483999\n",
      "Epoch: 0361 cost= 27.906016429\n",
      "Epoch: 0362 cost= 27.777918657\n",
      "Epoch: 0363 cost= 27.658573230\n",
      "Epoch: 0364 cost= 27.531592210\n",
      "Epoch: 0365 cost= 27.403363148\n",
      "Epoch: 0366 cost= 27.288665454\n",
      "Epoch: 0367 cost= 27.165313562\n",
      "Epoch: 0368 cost= 27.034769932\n",
      "Epoch: 0369 cost= 26.907619158\n",
      "Epoch: 0370 cost= 26.789824645\n",
      "Epoch: 0371 cost= 26.677474181\n",
      "Epoch: 0372 cost= 26.568909168\n",
      "Epoch: 0373 cost= 26.442646345\n",
      "Epoch: 0374 cost= 26.336716016\n",
      "Epoch: 0375 cost= 26.231855790\n",
      "Epoch: 0376 cost= 26.097319603\n",
      "Epoch: 0377 cost= 25.988656123\n",
      "Epoch: 0378 cost= 25.890530109\n",
      "Epoch: 0379 cost= 25.779914141\n",
      "Epoch: 0380 cost= 25.674184004\n",
      "Epoch: 0381 cost= 25.573523839\n",
      "Epoch: 0382 cost= 25.473556836\n",
      "Epoch: 0383 cost= 25.374543826\n",
      "Epoch: 0384 cost= 25.282115857\n",
      "Epoch: 0385 cost= 25.185657819\n",
      "Epoch: 0386 cost= 25.093890667\n",
      "Epoch: 0387 cost= 24.996941487\n",
      "Epoch: 0388 cost= 24.910847743\n",
      "Epoch: 0389 cost= 24.818933884\n",
      "Epoch: 0390 cost= 24.720457236\n",
      "Epoch: 0391 cost= 24.626337290\n",
      "Epoch: 0392 cost= 24.540075143\n",
      "Epoch: 0393 cost= 24.447279692\n",
      "Epoch: 0394 cost= 24.349652449\n",
      "Epoch: 0395 cost= 24.267521779\n",
      "Epoch: 0396 cost= 24.174115896\n",
      "Epoch: 0397 cost= 24.086227417\n",
      "Epoch: 0398 cost= 23.993507624\n",
      "Epoch: 0399 cost= 23.894258897\n",
      "Epoch: 0400 cost= 23.807745059\n",
      "Epoch: 0401 cost= 23.711708705\n",
      "Epoch: 0402 cost= 23.622955561\n",
      "Epoch: 0403 cost= 23.528574864\n",
      "Epoch: 0404 cost= 23.428301732\n",
      "Epoch: 0405 cost= 23.340533415\n",
      "Epoch: 0406 cost= 23.244481802\n",
      "Epoch: 0407 cost= 23.148572842\n",
      "Epoch: 0408 cost= 23.044703404\n",
      "Epoch: 0409 cost= 22.959074895\n",
      "Epoch: 0410 cost= 22.858140151\n",
      "Epoch: 0411 cost= 22.760248820\n",
      "Epoch: 0412 cost= 22.663907846\n",
      "Epoch: 0413 cost= 22.567174911\n",
      "Epoch: 0414 cost= 22.482398589\n",
      "Epoch: 0415 cost= 22.382232587\n",
      "Epoch: 0416 cost= 22.299526374\n",
      "Epoch: 0417 cost= 22.203611851\n",
      "Epoch: 0418 cost= 22.117965778\n",
      "Epoch: 0419 cost= 22.038483461\n",
      "Epoch: 0420 cost= 21.949920893\n",
      "Epoch: 0421 cost= 21.868143161\n",
      "Epoch: 0422 cost= 21.795952956\n",
      "Epoch: 0423 cost= 21.724075794\n",
      "Epoch: 0424 cost= 21.648024082\n",
      "Epoch: 0425 cost= 21.572769880\n",
      "Epoch: 0426 cost= 21.502798716\n",
      "Epoch: 0427 cost= 21.432381709\n",
      "Epoch: 0428 cost= 21.362892310\n",
      "Epoch: 0429 cost= 21.294928392\n",
      "Epoch: 0430 cost= 21.223940452\n",
      "Epoch: 0431 cost= 21.158631563\n",
      "Epoch: 0432 cost= 21.088083108\n",
      "Epoch: 0433 cost= 21.019167900\n",
      "Epoch: 0434 cost= 20.951764107\n",
      "Epoch: 0435 cost= 20.880414168\n",
      "Epoch: 0436 cost= 20.814359426\n",
      "Epoch: 0437 cost= 20.744409402\n",
      "Epoch: 0438 cost= 20.676979383\n",
      "Epoch: 0439 cost= 20.612436533\n",
      "Epoch: 0440 cost= 20.544124842\n",
      "Epoch: 0441 cost= 20.478274028\n",
      "Epoch: 0442 cost= 20.408775171\n",
      "Epoch: 0443 cost= 20.342778365\n",
      "Epoch: 0444 cost= 20.277010918\n",
      "Epoch: 0445 cost= 20.208165248\n",
      "Epoch: 0446 cost= 20.138433774\n",
      "Epoch: 0447 cost= 20.067731698\n",
      "Epoch: 0448 cost= 20.001495600\n",
      "Epoch: 0449 cost= 19.944474061\n",
      "Epoch: 0450 cost= 19.879287481\n",
      "Epoch: 0451 cost= 19.814408620\n",
      "Epoch: 0452 cost= 19.754711350\n",
      "Epoch: 0453 cost= 19.687063654\n",
      "Epoch: 0454 cost= 19.631560842\n",
      "Epoch: 0455 cost= 19.562177499\n",
      "Epoch: 0456 cost= 19.520822922\n",
      "Epoch: 0457 cost= 19.448179563\n",
      "Epoch: 0458 cost= 19.398316940\n",
      "Epoch: 0459 cost= 19.325985551\n",
      "Epoch: 0460 cost= 19.293168902\n",
      "Epoch: 0461 cost= 19.227641463\n",
      "Epoch: 0462 cost= 19.159110824\n",
      "Epoch: 0463 cost= 19.125693560\n",
      "Epoch: 0464 cost= 19.058769107\n",
      "Epoch: 0465 cost= 19.002295057\n",
      "Epoch: 0466 cost= 18.965925733\n",
      "Epoch: 0467 cost= 18.919522683\n",
      "Epoch: 0468 cost= 18.873029073\n",
      "Epoch: 0469 cost= 18.832026760\n",
      "Epoch: 0470 cost= 18.787197232\n",
      "Epoch: 0471 cost= 18.750541091\n",
      "Epoch: 0472 cost= 18.704197248\n",
      "Epoch: 0473 cost= 18.666931550\n",
      "Epoch: 0474 cost= 18.633746545\n",
      "Epoch: 0475 cost= 18.586698492\n",
      "Epoch: 0476 cost= 18.552672187\n",
      "Epoch: 0477 cost= 18.508857608\n",
      "Epoch: 0478 cost= 18.470229705\n",
      "Epoch: 0479 cost= 18.440323631\n",
      "Epoch: 0480 cost= 18.398358981\n",
      "Epoch: 0481 cost= 18.368821343\n",
      "Epoch: 0482 cost= 18.328502178\n",
      "Epoch: 0483 cost= 18.298383872\n",
      "Epoch: 0484 cost= 18.274625977\n",
      "Epoch: 0485 cost= 18.232080738\n",
      "Epoch: 0486 cost= 18.195889115\n",
      "Epoch: 0487 cost= 18.162451108\n",
      "Epoch: 0488 cost= 18.125307918\n",
      "Epoch: 0489 cost= 18.098533114\n",
      "Epoch: 0490 cost= 18.060708642\n",
      "Epoch: 0491 cost= 18.022396008\n",
      "Epoch: 0492 cost= 17.991880059\n",
      "Epoch: 0493 cost= 17.958598614\n",
      "Epoch: 0494 cost= 17.918739875\n",
      "Epoch: 0495 cost= 17.884360154\n",
      "Epoch: 0496 cost= 17.856536309\n",
      "Epoch: 0497 cost= 17.817374508\n",
      "Epoch: 0498 cost= 17.789175749\n",
      "Epoch: 0499 cost= 17.748298883\n",
      "Epoch: 0500 cost= 17.717059414\n",
      "Optimization Finished!\n",
      "Accuracy: 0.985333\n",
      "0.985333\n",
      "TEST RESULT:\n",
      "==============\n",
      "\n",
      "\n",
      "PRECISION:\n",
      "[ 0.99513551  0.97629725]\n",
      "\n",
      "\n",
      "RECALL:\n",
      "[ 0.9748128   0.99542782]\n",
      "\n",
      "\n",
      "F1-score:\n",
      "[ 0.98486933  0.98576973]\n",
      "\n",
      "\n",
      "SUPPORT:\n",
      "[1469 1531]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score\n",
    "import sklearn as sk\n",
    "from collections import Counter\n",
    "\n",
    "filename = \"qifeng_data_npfea_sep09_smote_30mfea.csv\"\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "data = open(filename, \"r\")\n",
    "count = 0\n",
    "line_count = 0\n",
    "neg_examples = 0\n",
    "pos_examples = 0\n",
    "for line in data:\n",
    "\tif line_count == 0:\n",
    "\t\tline_count += 1\n",
    "\t\tcontinue\n",
    "\n",
    "\tl = line.strip().split(',')\n",
    "\tx = []\n",
    "\t\n",
    "\tfor i in l[:-1]:\n",
    "\t\tx.append(float(i))\n",
    "\ty = float(l[-1])\n",
    "\n",
    "\tif y == 0.0:\n",
    "\t\tneg_examples += 1\n",
    "\telse:\n",
    "\t\tpos_examples += 1\n",
    "\n",
    "\n",
    "\tif pos_examples == 5000 and neg_examples == 5000:\n",
    "\t\tbreak\n",
    "\n",
    "\tif pos_examples < 5000 and y == 1.0:\n",
    "\t\tX.append(x)\n",
    "\t\tY.append(y)\n",
    "\n",
    "\tif neg_examples < 5000 and y == 0.0:\n",
    "\t\tX.append(x)\n",
    "\t\tY.append(y)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\t\t\n",
    "\n",
    "\n",
    "\n",
    "\t#if count > 1000:\n",
    "\t#\tbreak\n",
    "\tcount += 1\n",
    "\n",
    "\n",
    "\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "Y = np.array([Y, -(Y-1)]).T\n",
    "X, X_test, Y, Y_test = train_test_split(X, Y, test_size=0.30)\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.0001\n",
    "training_epochs = 500\n",
    "batch_size = 1000\n",
    "display_step = 1\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 70 # 1st layer number of features\n",
    "n_hidden_2 = 70 # 2nd layer number of features\n",
    "#n_hidden_3 = 50 # 2nd layer number of features\n",
    "#n_hidden_4 = 20 # 2nd layer number of features\n",
    "n_input = len(X[0]) # Number of feature\n",
    "n_classes = 2 # Number of classes to predict\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, n_input])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# Create model\n",
    "def multilayer_perceptron(x, weights, biases):\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "\n",
    "    # Hidden layer with RELU activation\n",
    "    #layer_3 = tf.add(tf.matmul(layer_2, weights['h3']), biases['b3'])\n",
    "    #layer_3 = tf.nn.relu(layer_3)\n",
    "    # Hidden layer with RELU activation\n",
    "    #layer_4 = tf.add(tf.matmul(layer_3, weights['h4']), biases['b4'])\n",
    "    #layer_4 = tf.nn.relu(layer_4)\n",
    "\n",
    "    # Output layer with linear activation\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    #out_layer = tf.matmul(layer_4, weights['out']) + biases['out']\n",
    "    return out_layer\n",
    "\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    #'h3': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_3])),\n",
    "    #'h4': tf.Variable(tf.random_normal([n_hidden_3, n_hidden_4])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes])),\n",
    "    #'out': tf.Variable(tf.random_normal([n_hidden_4, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    #'b3': tf.Variable(tf.random_normal([n_hidden_3])),\n",
    "    #'b4': tf.Variable(tf.random_normal([n_hidden_4])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "# Construct model\n",
    "pred = multilayer_perceptron(x, weights, biases)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))\n",
    "#cross_entropy = tf.reduce_sum(- y * tf.log(pred) - (1 - y) * tf.log(1 - pred), 1)\n",
    "#cost = tf.reduce_mean(-tf.reduce_sum(pred * tf.log(y), reduction_indices=[1]))\n",
    "#cross_entropy = tf.reduce_sum(pred*tf.log(y + 1e-10))\n",
    "#cost = tf.reduce_mean(cross_entropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(len(X)/batch_size)\n",
    "        X_batches = np.array_split(X, total_batch)\n",
    "        Y_batches = np.array_split(Y, total_batch)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = X_batches[i], Y_batches[i]\n",
    "            # batch_y.shape = (batch_y.shape[0], 1)\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={x: batch_x,\n",
    "                                                          y: batch_y})\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print \"Epoch:\", '%04d' % (epoch+1), \"cost=\", \\\n",
    "                \"{:.9f}\".format(avg_cost)\n",
    "    print \"Optimization Finished!\"\n",
    "\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print \"Accuracy:\", accuracy.eval({x: X_test, y: Y_test})\n",
    "    global result \n",
    "    result = tf.argmax(pred, 1).eval({x: X_test, y: Y_test})\n",
    "\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print(sess.run(accuracy, feed_dict={x: X_test, y: Y_test}))\n",
    "\n",
    "    y_p = tf.argmax(pred, 1)\n",
    "\n",
    "    #prediction_probab = pred/tf.reduce_sum(pred,1)\n",
    "\n",
    "\n",
    "    #print (\"may be probabilities \"), prediction_probab.eval(session=sess,feed_dict={x:pred}) #prediction_probab.eval()\n",
    "\n",
    "\n",
    "    val_accuracy, y_pred = sess.run([accuracy, y_p], feed_dict={x:X_test, y:Y_test})\n",
    "\n",
    "    Y_test = [not i for i in Y_test[:,0]]    \n",
    "    y_pred =  y_pred.tolist()\n",
    "\n",
    "    (test_precision,test_recall,test_fscore,test_support)=precision_recall_fscore_support(Y_test, y_pred, beta=1.0, labels=None,\n",
    "\t                            pos_label=1, average=None,\n",
    "\t                            warn_for=('precision', 'recall',\n",
    "\t                                      'f-score'),\n",
    "\t                            sample_weight=None)\n",
    "\n",
    "    Test = (test_precision,test_recall,test_fscore,test_support)\n",
    "    print \"TEST RESULT:\"\n",
    "    print\"==============\"\n",
    "    print \"\\n\"\n",
    "    print \"PRECISION:\"\n",
    "    print(Test[0])\n",
    "    print \"\\n\"\n",
    "    print \"RECALL:\"\n",
    "    print(Test[1])\n",
    "    print \"\\n\"\n",
    "    print \"F1-score:\"\n",
    "    print(Test[2])\n",
    "    print \"\\n\"\t\n",
    "    print \"SUPPORT:\"\n",
    "    print(Test[3])\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
